{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7febd7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e7874f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] =  os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d997d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001BB3A70D8D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BB3A70CF10>, root_client=<openai.OpenAI object at 0x000001BB3A70DBA0>, root_async_client=<openai.AsyncOpenAI object at 0x000001BB3A70D870>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8dd70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input and get response from llm\n",
    "result = llm.invoke(\"What is generative ai?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faad88b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Generative AI refers to a category of artificial intelligence systems designed to generate new content, including text, images, audio, and video, that can resemble human-created content. These systems use models trained on large datasets to learn patterns and structures within the data, which they can then use to produce novel outputs. Common types of generative AI models include:\\n\\n1. **Generative Adversarial Networks (GANs)**: These consist of two neural networks—the generator and the discriminator—that are trained simultaneously. The generator creates new data instances, while the discriminator evaluates them. Both networks improve through this adversarial process, enhancing the realism of the generated content.\\n\\n2. **Variational Autoencoders (VAEs)**: These models encode input data into a latent space and then decode it back to generate new content. VAEs are often used for generating complex data distributions, as they enable more structured and controlled generation.\\n\\n3. **Transformers and Large Language Models (LLMs)**: These models, such as OpenAI's GPT (Generative Pre-trained Transformer) series, leverage transformer architectures to generate human-like text. They are pre-trained on massive text datasets and fine-tuned for specific tasks or contexts.\\n\\nGenerative AI has numerous applications, including content creation for marketing, art and music generation, improved tools in natural language processing, video game design, and even scientific research where it can simulate molecules or predict protein structures. However, it also raises ethical and legal concerns, such as the potential for copyright infringements, deepfakes, and the spread of misinformation.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 313, 'prompt_tokens': 13, 'total_tokens': 326, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BqmHWWEzOynvTaBZkO6Hwol1SoUyN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--49fd6231-aac0-48e5-9539-bac1c1f9e5f1-0' usage_metadata={'input_tokens': 13, 'output_tokens': 313, 'total_tokens': 326, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a51bda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "       (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions.\"),\n",
    "       (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b25c60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangSmith is a tool developed by the team behind LangChain, designed to enhance the workflow for applications utilizing large language models (LLMs). It provides a suite of capabilities to assist developers in building, debugging, and monitoring LLM-based applications more efficiently.\\n\\nKey features of LangSmith include:\\n\\n1. **Testing and Debugging**: LangSmith offers testing frameworks to evaluate the performance of LLM applications. It helps identify and analyze errors or inefficiencies in how language models handle tasks or instructions.\\n\\n2. **Evaluation**: The platform supports both quantitative and qualitative evaluations of language model outputs, allowing developers to assess them against various metrics and standards.\\n\\n3. **Tracing**: LangSmith provides tracing features that track the flow of data and the behavior of models across different stages of processing. This is crucial for understanding how applications process inputs and produce outputs.\\n\\n4. **Monitoring**: It includes monitoring tools to keep track of application performance in real-time, enabling quick detection of issues or anomalies in LLM operations.\\n\\n5. **Integration**: LangSmith is designed to integrate seamlessly with LangChain and other tools, creating a coherent environment for developing advanced applications.\\n\\nOverall, LangSmith acts as a comprehensive platform for refining the use of language models in various applications, thus supporting developers from the initial build phase through to deployment and scaling.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 265, 'prompt_tokens': 33, 'total_tokens': 298, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BqmPxAd7RoqQULV04fE74bYbVV5Ha', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--59bb6aac-7bd4-4354-8b81-d2c8f9e7d211-0', usage_metadata={'input_tokens': 33, 'output_tokens': 265, 'total_tokens': 298, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about langsmith\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "137178be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith is a tool associated with LangChain, which is a framework designed for building applications that integrate with large language models (LLMs). LangSmith focuses on providing robust debugging and testing capabilities for applications that utilize LLMs and AI agents. Its primary goal is to streamline the process of developing, optimizing, and maintaining applications that rely on LLMs by offering enhanced insights and diagnostics.\n",
      "\n",
      "Key features of LangSmith include:\n",
      "\n",
      "1. **Tracing and Debugging**: It offers tools to trace the execution of applications using LangChain, allowing developers to debug and understand how their applications interact with language models.\n",
      "\n",
      "2. **Testing Capabilities**: LangSmith provides frameworks for testing applications to ensure they behave as expected. This includes validating model outputs and performance.\n",
      "\n",
      "3. **Performance Optimization**: By analyzing the interactions and bottlenecks within applications, LangSmith helps optimize performance, making the applications more efficient and reliable.\n",
      "\n",
      "4. **Analytics and Logging**: It offers comprehensive logging and analytics features to monitor the performance and usage of LLMs, giving developers insights into how their applications are being used and how they can be improved.\n",
      "\n",
      "Overall, LangSmith is a valuable tool for developers building sophisticated AI applications with LangChain, focusing on debugging, testing, and optimizing LLM-powered workflows.\n"
     ]
    }
   ],
   "source": [
    "## String output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about langsmith\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a8c11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b7de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
